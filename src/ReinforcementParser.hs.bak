{-# LANGUAGE DataKinds #-}
{-# LANGUAGE DeriveAnyClass #-}
{-# LANGUAGE DerivingStrategies #-}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE PartialTypeSignatures #-}
{-# LANGUAGE RecordWildCards #-}
{-# HLINT ignore "Use <$>" #-}
{-# OPTIONS_GHC -Wno-unrecognised-pragmas #-}

module ReinforcementParser where

import Common
import Control.DeepSeq (force)
import Control.Exception (Exception, catch, onException)
import Control.Foldl qualified as Foldl
import Control.Monad (foldM, foldM_, forM_, replicateM, when)
import Control.Monad.Except qualified as ET
import Control.Monad.Primitive (RealWorld)
import Control.Monad.State qualified as ST
import Control.Monad.Trans (lift)
import Data.Foldable qualified as F
import Data.HashSet qualified as HS
import Data.List.Extra qualified as E
import Data.Maybe (catMaybes)
import Data.Vector qualified as V
import Debug.Trace qualified as DT
import Display (replayDerivation, viewGraph)
import GHC.Generics (Generic)
import Graphics.Rendering.Chart.Backend.Cairo as Plt
import Graphics.Rendering.Chart.Easy ((.=))
import Graphics.Rendering.Chart.Easy qualified as Plt
import Graphics.Rendering.Chart.Gtk qualified as Plt
import GreedyParser (Action, ActionDouble (ActionDouble), ActionSingle (ActionSingle), GreedyState, Trans (Trans), getActions, initParseState, parseGreedy, parseStep, pickRandom)
import Inference.Conjugate (HyperRep, Prior (expectedProbs), evalTraceLogP, sampleProbs)
import Internal.MultiSet qualified as MS
import Musicology.Pitch
import PVGrammar (Edge, Edges (Edges), Freeze (FreezeOp), Notes (Notes), PVAnalysis, PVLeftmost, Split, Spread)
import PVGrammar.Generate (derivationPlayerPV)
import PVGrammar.Parse (protoVoiceEvaluator)
import PVGrammar.Prob.Simple (PVParams, observeDerivation, observeDerivation', sampleDerivation')
import System.Random (RandomGen, getStdRandom)
import System.Random.MWC.Distributions (categorical)
import System.Random.MWC.Probability qualified as MWC
import System.Random.Shuffle (shuffle')
import System.Random.Stateful as Rand (StatefulGen, UniformRange (uniformRM), split)
import Torch qualified as T
import Torch.Lens qualified
import Torch.Typed qualified as TT

-- Notes
-- -----

{-
Variant of Q-learning:
- instead of Q value (expected total reward) under optimal policy
  learn "P value": expected probability under random policy
- does this lead to a policy where p(as) âˆ reward?
  - then you learn a method of sampling from the reward distribution
  - if reward is a probability (e.g. p(deriv)), you learn to sample from that!
    - useful for unsupervised inference
- changes:
  - use proportional random policy (is this MC-tree-search?)
  - loss uses E[] instead of max over next actions.
-}

-- global settings
-- ---------------

device :: T.Device
-- device = T.Device T.CUDA 0
device = T.Device T.CPU 0

type QDevice = 'TT.CPU

opts :: T.TensorOptions
opts = T.withDType T.Double $ T.withDevice device T.defaultOpts

toOpts :: forall a. (Torch.Lens.HasTypes a T.Tensor) => a -> a
toOpts = T.toDevice device . T.toType T.Double

-- discount factor
gamma :: T.Tensor
-- gamma = toOpts $ T.asTensor @Double 0.99
gamma = 0.99

-- interpolation factor between target and policy net
tau :: T.Tensor
-- tau = toOpts $ T.asTensor @Double 0.05
tau = 0.05

learningRate :: T.LearningRate
-- learningRate = toOpts $ T.asTensor @Double 0.01
learningRate = 0.001

-- replay buffer
bufferSize :: Int
bufferSize = 10_000

replayN :: Int
replayN = 200

-- exploration factors
epsStart :: Double
epsStart = 0.9

epsEnd :: Double
epsEnd = 0.05

epsDecay :: Double
epsDecay = 1000

eps :: Int -> Double
eps i = epsEnd + (epsStart - epsEnd) * exp (negate (fromIntegral i) / epsDecay)

-- device = T.Device T.CPU 0

-- States and Actions
-- ------------------

newtype RPState tr tr' slc s f h = RPState (GreedyState tr tr' slc (Leftmost s f h))
  deriving (Show)

newtype RPAction slc tr s f h = RPAction (Action slc tr s f h)
  deriving (Show)

-- Replay Buffer
-- -------------

data ReplayStep tr tr' slc s f h r = ReplayStep
  { _state :: !(RPState tr tr' slc s f h)
  , _action :: !(RPAction slc tr s f h)
  , _nextState :: !(Maybe (RPState tr tr' slc s f h))
  , _reward :: !r
  }
  deriving (Show)

data ReplayBuffer tr tr' slc s f h r
  = ReplayBuffer !Int ![ReplayStep tr tr' slc s f h r]
  deriving (Show)

mkReplayBuffer :: Int -> ReplayBuffer tr tr' slc s f h r
mkReplayBuffer n = ReplayBuffer n []

pushStep
  :: ReplayBuffer tr tr' slc s f h r
  -> ReplayStep tr tr' slc s f h r
  -> ReplayBuffer tr tr' slc s f h r
pushStep (ReplayBuffer n queue) trans = ReplayBuffer n $ take n $ trans : queue

sampleSteps
  :: ReplayBuffer tr tr' slc s f h r
  -> Int
  -> IO [ReplayStep tr tr' slc s f h r]
sampleSteps (ReplayBuffer _ queue) n = do
  -- not great, but shuffle' doesn't integrated with StatefulGen
  gen <- getStdRandom Rand.split
  pure $ take n (shuffle' queue (length queue) gen)

-- Encoding
-- --------

newtype QEncoding = QEncoding
  { _actionEncoding :: (Either SingleTop DoubleTop, Leftmost () () ())
  }
  deriving (Show)

type PVAction = Action (Notes SPitch) (Edges SPitch) (Split SPitch) Freeze (Spread SPitch)

pitch2index :: GeneralSpec -> SPitch -> [Int]
pitch2index GeneralSpec{..} p = [fifths p - fifthLow, octaves p - octaveLow]

pitchesOneHot :: GeneralSpec -> HS.HashSet SPitch -> T.Tensor
pitchesOneHot spec@GeneralSpec{..} ps = T.toDense $ T.sparseCooTensor indices values dims opts
 where
  indices = T.transpose2D $ T.asTensor' (pitch2index spec <$> F.toList ps) (T.withDType T.Int64 opts)
  values = T.ones [F.length ps] opts
  dims = [fifthSize, octaveSize]

encodeSlice :: GeneralSpec -> Notes SPitch -> TT.Tensor dev 'TT.Double []
encodeSlice spec (Notes notes) =
  -- DT.trace ("ecoding slice" <> show notes) $
  pitchesOneHot spec $ MS.toSet notes

encodeTransition :: GeneralSpec -> Edges SPitch -> T.Tensor
encodeTransition _spec (Edges _reg _pass) = T.asTensor @Double 0

type SingleTop = (StartStop T.Tensor, T.Tensor, StartStop T.Tensor)
type DoubleTop = (StartStop T.Tensor, T.Tensor, T.Tensor, T.Tensor, StartStop T.Tensor)

type ActionEncoding = (Either SingleTop DoubleTop, Leftmost () () ())

encodePVAction :: GeneralSpec -> PVAction -> ActionEncoding
encodePVAction spec (Left (ActionSingle top action)) = (encTop, encAction)
 where
  (sl, GreedyParser.Trans t _2nd, sr) = top
  encTop = Left (encodeSlice spec <$> sl, encodeTransition spec t, encodeSlice spec <$> sr)
  encAction = case action of
    LMSingleFreeze FreezeOp -> LMFreezeOnly ()
    LMSingleSplit _split -> LMSplitOnly ()
encodePVAction spec (Right (ActionDouble top action)) = (encTop, encAction)
 where
  (sl, GreedyParser.Trans t1 _, sm, Trans t2 _, sr) = top
  encTop =
    Right
      ( encodeSlice spec <$> sl
      , encodeTransition spec t1
      , encodeSlice spec sm
      , encodeTransition spec t2
      , encodeSlice spec <$> sr
      )
  encAction = case action of
    LMDoubleFreezeLeft FreezeOp -> LMFreezeLeft ()
    LMDoubleSplitLeft split -> LMSplitLeft ()
    LMDoubleSplitRight split -> LMSplitRight ()
    LMDoubleSpread spread -> LMSpread ()

encodeStep :: GeneralSpec -> p -> PVAction -> QEncoding
encodeStep spec _ action = QEncoding (encodePVAction spec action)

-- Q net
-- -----

-- General Spec

data GeneralSpec = GeneralSpec
  { fifthLow :: Int
  , fifthSize :: Int
  , octaveLow :: Int
  , octaveSize :: Int
  , embSize :: Int
  }

-- helper: ConstEmb

newtype ConstEmbSpec = ConstEmbSpec [Int]

-- TODO: fix this to learn something (activation?)
newtype ConstEmb = ConstEmb T.Parameter
  deriving (Show, Generic)
  deriving anyclass (T.Parameterized)

instance T.Randomizable ConstEmbSpec ConstEmb where
  sample :: ConstEmbSpec -> IO ConstEmb
  -- sample (ConstEmbSpec shape) = pure $ ConstEmb $ T.zeros shape opts
  sample (ConstEmbSpec shape) = do
    values <- T.randIO shape opts
    let values' = 2 * values - 1
    init' <- T.makeIndependent values'
    pure $ ConstEmb init'

instance T.HasForward ConstEmb () T.Tensor where
  forward (ConstEmb emb) () = T.toDependent emb
  forwardStoch model input = pure $ T.forward model input

-- Slice Encoder

newtype SliceSpec = SliceSpec {slcNHidden :: Int}

data SliceEncoder = SliceEncoder
  { _slcL1 :: !T.Linear
  , _slcL2 :: !T.Linear
  , _slcStart :: !ConstEmb
  , _slcStop :: !ConstEmb
  }
  deriving (Show, Generic, T.Parameterized)

instance T.Randomizable (GeneralSpec, SliceSpec) SliceEncoder where
  sample :: (GeneralSpec, SliceSpec) -> IO SliceEncoder
  sample (GeneralSpec{..}, SliceSpec hidden) =
    SliceEncoder
      <$> (toOpts <$> T.sample (T.LinearSpec (fifthSize * octaveSize) hidden))
      <*> (toOpts <$> T.sample (T.LinearSpec hidden embSize))
      <*> T.sample (ConstEmbSpec [embSize])
      <*> T.sample (ConstEmbSpec [embSize])

instance T.HasForward SliceEncoder (StartStop T.Tensor) T.Tensor where
  forward model@(SliceEncoder _ _ start stop) input =
    case input of
      Inner slc -> T.forward model slc
      Start -> T.forward start ()
      Stop -> T.forward stop ()
  forwardStoch model input = pure $ T.forward model input

instance T.HasForward SliceEncoder T.Tensor T.Tensor where
  forward (SliceEncoder l1 l2 start stop) =
    T.relu . T.linear l2 . T.relu . T.linear l1 . T.flattenAll
  forwardStoch model = pure . T.forward model

-- ActionEncoder

data ActionSpec = ActionSpec
  { actNHidden :: Int
  }

data ActionEncoder = ActionEncoder
  { actTop1_2 :: T.Linear
  , actTop1_3 :: T.Linear
  , actTop2 :: T.Linear
  , actSplit :: ConstEmb -- TODO: fill in with actual module
  , actSpread :: ConstEmb -- TODO: fill in with actual module
  , actFreeze :: ConstEmb
  }
  deriving (Show, Generic, T.Parameterized)

instance T.Randomizable (GeneralSpec, ActionSpec) ActionEncoder where
  sample :: (GeneralSpec, ActionSpec) -> IO ActionEncoder
  sample (GeneralSpec{embSize}, ActionSpec hidden) =
    ActionEncoder
      <$> (toOpts <$> T.sample (T.LinearSpec (2 * embSize) hidden))
      <*> (toOpts <$> T.sample (T.LinearSpec (3 * embSize) hidden))
      <*> (toOpts <$> T.sample (T.LinearSpec hidden embSize))
      <*> (toOpts <$> T.sample (ConstEmbSpec [embSize - 3]))
      <*> (toOpts <$> T.sample (ConstEmbSpec [embSize - 3]))
      <*> (toOpts <$> T.sample (ConstEmbSpec [embSize - 3]))

instance T.HasForward ActionEncoder (SliceEncoder, ActionEncoding) T.Tensor where
  forward ActionEncoder{..} (slc, (top, op)) = T.cat (T.Dim 0) [topEmb, opEmb]
   where
    topEmb = T.linear actTop2 $ T.relu $ case top of
      Left (sl, _t, sr) -> T.linear actTop1_2 $ T.cat (T.Dim 0) [T.forward slc sl, T.forward slc sr]
      Right (sl, _t1, sm, _t2, sr) -> T.linear actTop1_3 $ T.cat (T.Dim 0) [T.forward slc sl, T.forward slc sm, T.forward slc sr]
    opEmb = case op of
      LMFreezeOnly _ -> T.cat (T.Dim 0) [opTypes T.! (0 :: Int), T.forward actFreeze ()]
      LMSplitOnly _ -> T.cat (T.Dim 0) [opTypes T.! (1 :: Int), T.forward actSplit ()]
      LMFreezeLeft _ -> T.cat (T.Dim 0) [opTypes T.! (2 :: Int), T.forward actFreeze ()]
      LMSpread _ -> T.cat (T.Dim 0) [opTypes T.! (3 :: Int), T.forward actSpread ()]
      LMSplitLeft _ -> T.cat (T.Dim 0) [opTypes T.! (4 :: Int), T.forward actSplit ()]
      LMSplitRight _ -> T.cat (T.Dim 0) [opTypes T.! (5 :: Int), T.forward actSplit ()]
    opTypes =
      toOpts $
        T.asTensor @[[Double]]
          [ [0, 0, 0] -- freeze only
          , [0, 1, 0] -- split only
          , [1, 0, 0] -- freeze left
          , [1, 0, 1] -- spread
          , [1, 1, 0] -- freeze left
          , [1, 1, 1] -- freeze right
          ]
  forwardStoch a i = pure $ T.forward a i

-- Full Model

newtype SpecialSpec = SpecialSpec
  {finalNHidden :: Int}

data QSpec = QSpec GeneralSpec SpecialSpec SliceSpec ActionSpec

defaultSpec :: QSpec
defaultSpec =
  QSpec
    defaultGSpec
    SpecialSpec{finalNHidden = 32}
    SliceSpec{slcNHidden = 32}
    ActionSpec{actNHidden = 32}

defaultGSpec :: GeneralSpec
defaultGSpec =
  GeneralSpec
    { fifthLow = -3
    , fifthSize = 12
    , octaveLow = 2
    , octaveSize = 5
    , embSize = 16
    }

data QModel = QModel
  { qModelSlc :: !SliceEncoder
  , qModelAct :: !ActionEncoder
  , qModelFinal1 :: !T.Linear
  , qModelFinal2 :: !T.Linear
  }
  deriving (Show, Generic, T.Parameterized)

instance T.Randomizable QSpec QModel where
  sample :: QSpec -> IO QModel
  sample (QSpec gspec sspec slcspec actspec) =
    QModel
      <$> T.sample (gspec, slcspec)
      <*> T.sample (gspec, actspec)
      <*> (toOpts <$> T.sample (T.LinearSpec (embSize gspec * 2) (finalNHidden sspec)))
      <*> (toOpts <$> T.sample (T.LinearSpec (finalNHidden sspec) 1))

instance T.HasForward QModel QEncoding T.Tensor where
  forward :: QModel -> QEncoding -> T.Tensor
  forward (QModel slc act final1 final2) (QEncoding actEnc) =
    T.linear final2 $ T.relu $ T.linear final1 actEmb
   where
    actEmb = T.forward act (slc, actEnc)

  forwardStoch :: QModel -> QEncoding -> IO T.Tensor
  forwardStoch model input = pure $ T.forward model input

{- | A loss for any model with 0 gradients everywhere.
Can be used to ensure that all parameters have a gradient,
if not all parameters are used in the real loss.
-}
fakeLoss :: (T.Parameterized f) => f -> T.Tensor
fakeLoss model = tzero * sum (T.sumAll . T.toDependent <$> T.flattenParameters model)
 where
  tzero = toOpts $ T.asTensor @Double 0

mkQModel :: QSpec -> IO QModel
mkQModel = T.sample

runQ !encode !model s a = T.asValue $ T.forward model $ encode s a

-- Reward
-- ------

inf :: Double
inf = 1 / 0

pvRewardSample
  :: MWC.Gen RealWorld
  -> PVParams HyperRep
  -> PVAnalysis SPitch
  -> IO Double
pvRewardSample gen hyper (Analysis deriv top) = do
  let trace = observeDerivation deriv top
  probs <- MWC.sample (sampleProbs @PVParams hyper) gen
  case trace of
    Left error -> do
      putStrLn $ "error giving reward: " <> error
      pure (-inf)
    Right trace -> case evalTraceLogP probs trace sampleDerivation' of
      Nothing -> do
        putStrLn "Couldn't evaluate trace while giving reward"
        pure (-inf)
      Just (_, logprob) -> pure logprob

pvRewardExp :: PVParams HyperRep -> PVAnalysis SPitch -> IO Double
pvRewardExp hyper (Analysis deriv top) =
  case trace of
    Left error -> do
      putStrLn $ "error giving reward: " <> error
      pure (-inf)
    Right trace -> case evalTraceLogP probs trace sampleDerivation' of
      Nothing -> do
        putStrLn "Couldn't evaluate trace while giving reward"
        pure (-inf)
      Just (_, logprob) -> pure logprob
 where
  probs = expectedProbs @PVParams hyper
  trace = observeDerivation deriv top

-- Deep Q-Learning
-- ---------------

data DQNState opt tr tr' slc s f h r = DQNState
  { pnet :: !QModel
  , tnet :: !QModel
  , opt :: !opt
  , buffer :: !(ReplayBuffer tr tr' slc s f h r)
  }

epsilonGreedyPolicy
  :: (StatefulGen gen m)
  => gen
  -> Double
  -> (state -> action -> Double)
  -> state
  -> [action]
  -> m action
epsilonGreedyPolicy gen epsilon q state actions = do
  coin <- uniformRM (0, 1) gen
  if coin >= epsilon
    then pure $ E.maximumOn (q state) actions
    else do
      i <- uniformRM (0, length actions - 1) gen
      pure $ actions !! i

greedyPolicy
  :: (Applicative m)
  => (state -> action -> Double)
  -> state
  -> [action]
  -> m action
greedyPolicy q state actions = do
  pure $ E.maximumOn (q state) actions

softmaxPolicy
  :: (StatefulGen gen m)
  => gen
  -> (state -> action -> Double)
  -> state
  -> [action]
  -> m action
softmaxPolicy gen q state actions = do
  let probs = T.softmax (T.Dim 0) $ T.asTensor $ q state <$> actions
  actionIndex <- categorical (V.fromList $ T.asValue probs) gen
  pure $ actions !! actionIndex

runEpisode
  :: forall tr tr' slc slc' s f h gen
   . Eval tr tr' slc slc' (Leftmost s f h)
  -> ( GreedyState tr tr' slc (Leftmost s f h)
       -> [Action slc tr s f h]
       -> IO (Action slc tr s f h)
     )
  -> Path slc' tr'
  -> IO
      ( Either
          String
          ( [ ( GreedyState tr tr' slc (Leftmost s f h)
              , Action slc tr s f h
              , Maybe (GreedyState tr tr' slc (Leftmost s f h))
              )
            ]
          , Analysis s f h tr slc
          )
      )
runEpisode !eval !policyF !input =
  ST.evalStateT (ET.runExceptT $ go [] $ initParseState eval input) Nothing
 where
  go transitions state = do
    ST.put Nothing -- TODO: have parseStep return the action instead of using State
    result <- parseStep eval policy state
    action <- ST.get
    case result of
      Right (top, deriv) -> pure (addStep action Nothing transitions, Analysis deriv $ PathEnd top)
      Left state' -> go (addStep action (Just state') transitions) state'
   where
    addStep Nothing _state' ts = ts
    addStep (Just action) state' ts = (state, action, state') : ts

    policy :: [Action slc tr s f h] -> ET.ExceptT String (ST.StateT (Maybe (Action slc tr s f h)) IO) (Action slc tr s f h)
    policy [] = ET.throwError "no actions to select from"
    policy actions = do
      action <- lift $ lift $ policyF state actions
      ST.put (Just action)
      pure action

trainLoop
  :: forall tr tr' slc slc' s f h gen opt
   . ( StatefulGen gen IO
     , T.Optimizer opt
     , Show opt
     )
  => gen
  -> Eval tr tr' slc slc' (Leftmost s f h)
  -> (GreedyState tr tr' slc (Leftmost s f h) -> Action slc tr s f h -> QEncoding)
  -> (Analysis s f h tr slc -> IO Double)
  -> Path slc' tr'
  -> DQNState opt tr tr' slc s f h Double
  -> Int
  -> IO (DQNState opt tr tr' slc s f h Double, Double, Double)
trainLoop !gen !eval !encode !reward !piece oldstate@(DQNState !pnet !tnet !opt !buffer) i = do
  -- 1. run episode, collect results
  result <- runEpisode eval (softmaxPolicy gen $ runQ encode pnet) piece
  case result of
    -- error? skip
    Left error -> do
      print error
      pure (oldstate, 0, 0)
    Right (steps, analysis) -> do
      -- 2. compute reward and add steps to replay buffer
      r <- reward analysis
      let steps' = case steps of
            [] -> []
            last : rest -> mkStep r last : fmap (mkStep 0) rest
          buffer' = F.foldl' pushStep buffer steps'
      -- 3. optimize models
      (pnet', tnet', opt', loss) <- optimizeModels buffer'
      pure (DQNState pnet' tnet' opt' buffer', r, loss)
 where
  mkStep r (state, action, state') =
    ReplayStep (RPState state) (RPAction action) (RPState <$> state') r

  -- A single optimization step for deep q learning (DQN)
  optimizeModels buffer' = do
    -- choose batch from replay buffer
    batch <- sampleSteps buffer' replayN
    -- compute loss over batch
    let (qsNow, qsExpected) = unzip (dqnValues <$> batch)
    expectedDetached <- T.detach $ T.stack (T.Dim 0) qsExpected
    let !loss =
          T.l1Loss
            T.ReduceMean
            (T.stack (T.Dim 0) qsNow)
            expectedDetached
    -- print loss
    -- optimize policy net
    (pnet', opt') <- T.runStep pnet opt (loss + fakeLoss pnet) learningRate
    -- update target net
    tparams <- mapM T.detach $ T.toDependent <$> T.flattenParameters tnet
    pparams <- mapM T.detach $ T.toDependent <$> T.flattenParameters pnet'
    let interpolate p t = tau * p + (1 - tau) * t
         where
          p' = DT.traceShow (T.sumAll p) p
          t' = DT.traceShow (T.sumAll p) t
        tparams' = zipWith interpolate pparams tparams
    tparamsNew <- mapM T.makeIndependent tparams'
    let summarizeQ q = show $ T.sumAll . T.toDependent <$> T.flattenParameters q
        tnet' = T.replaceParameters tnet tparamsNew
    -- return new state
    pure (pnet', tnet', opt', T.asValue loss)

  -- The loss function of a single replay step
  dqnValues (ReplayStep (RPState s) (RPAction a) s' r) = (qnow, qexpected)
   where
    qzero = T.asTensor' [0 :: Double] opts
    qnext = case s' of
      Nothing -> qzero
      Just (RPState state') ->
        let
          nextQs = T.forward tnet . encode state' <$> getActions eval state'
         in
          E.maximumOn (T.asValue @Double) nextQs
    qnow = T.forward pnet (encode s a)
    qexpected = T.addScalar r (gamma * qnext)

-- delta = qnow - qexpected

trainDQN
  :: forall gen tr tr' slc slc' s f h
   . (StatefulGen gen IO, Show s, Show f, Show h)
  => gen
  -> Eval tr tr' slc slc' (Leftmost s f h)
  -> (GreedyState tr tr' slc (Leftmost s f h) -> Action slc tr s f h -> QEncoding)
  -> (Analysis s f h tr slc -> IO Double)
  -> [Path slc' tr']
  -> Int
  -> IO ([Double], [Double])
trainDQN gen eval encode reward pieces n = do
  model0 <- mkQModel defaultSpec
  let opt = T.mkAdam 0 0.9 0.99 (T.flattenParameters model0) -- T.GD
      buffer = mkReplayBuffer bufferSize
      state0 = DQNState model0 model0 opt buffer
  (DQNState modelTrained _ _ _, rewards, losses, accs) <- T.foldLoop (state0, [], [], []) n trainEpoch
  pure (reverse rewards, reverse losses) -- (modelTrained, rewards)
 where
  trainPiece i (state, rewards, losses) piece = do
    (state', r, loss) <- trainLoop gen eval encode reward piece state i
    pure (state', r : rewards, loss : rewards)
  trainEpoch (state, meanRewards, meanLosses, accuracies) i = do
    -- run epoch
    (state', rewards, losses) <-
      foldM (trainPiece i) (state, [], []) pieces
    let meanRewards' = Foldl.fold Foldl.mean rewards : meanRewards
        meanLosses' = Foldl.fold Foldl.mean losses : meanLosses
    -- compute greedy reward ("accuracy")
    accuracies' <-
      if (i `mod` 10) == 0
        then do
          results <- mapM (runEpisode eval $ greedyPolicy (runQ encode (pnet state'))) pieces
          case sequence results of
            Left error -> do
              putStrLn error
              pure $ (-inf) : accuracies
            Right episodes -> do
              accs <- mapM (reward . snd) episodes
              pure $ Foldl.fold Foldl.mean accs : accuracies
        else pure accuracies
    -- logging
    when ((i `mod` 10) == 0) $ do
      putStrLn $ "epoch " <> show i
      plotHistory "rewards" $ reverse meanRewards'
      plotHistory "losses" $ reverse meanLosses'
      plotHistory "accuracy" $ reverse accuracies'
    -- when ((i `mod` 100) == 0) $ do
    --   let DQNState{pnet} = state'
    --       q !s !a = T.asValue $ T.forward pnet $ encode s a
    --   results <-
    --     replicateM 100 $
    --       mapM
    --         ( runEpisode eval $ softmaxPolicy gen q
    --         -- epsilonGreedyPolicy gen (eps i) q
    --         )
    --         pieces
    -- forM_ (snd <$> episodes) $ \(Analysis deriv _) -> do
    --   putStrLn "average derivation currently:"
    --   mapM_ print deriv
    pure (state', meanRewards', meanLosses', accuracies')

-- Plotting
-- --------

mkHistoryPlot
  :: String
  -> [Double]
  -> ST.StateT
      (Plt.Layout Int Double)
      (ST.State Plt.CState)
      ()
mkHistoryPlot title values = do
  Plt.setColors $ Plt.opaque <$> [Plt.steelblue]
  Plt.layout_title .= title
  Plt.plot $ Plt.line title [points]
 where
  points = zip [1 :: Int ..] values

showHistory :: String -> [Double] -> IO ()
showHistory title values = Plt.toWindow 60 40 $ mkHistoryPlot title values

plotHistory :: String -> [Double] -> IO ()
plotHistory title values = Plt.toFile Plt.def (title <> ".svg") $ mkHistoryPlot title values

plotDeriv :: (Foldable t) => FilePath -> t (Leftmost (Split SPitch) Freeze (Spread SPitch)) -> IO ()
plotDeriv fn deriv = do
  case replayDerivation derivationPlayerPV deriv of
    (Left err) -> putStrLn err
    (Right g) -> viewGraph fn g

hi s = putStrLn $ "Found the Exception:" <> s
